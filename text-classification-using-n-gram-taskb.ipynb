{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e90aea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:47.630636Z",
     "iopub.status.busy": "2022-09-14T15:23:47.629124Z",
     "iopub.status.idle": "2022-09-14T15:23:49.516834Z",
     "shell.execute_reply": "2022-09-14T15:23:49.515476Z"
    },
    "papermill": {
     "duration": 1.90784,
     "end_time": "2022-09-14T15:23:49.519881",
     "exception": false,
     "start_time": "2022-09-14T15:23:47.612041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf20e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:49.555478Z",
     "iopub.status.busy": "2022-09-14T15:23:49.555097Z",
     "iopub.status.idle": "2022-09-14T15:23:50.108929Z",
     "shell.execute_reply": "2022-09-14T15:23:50.107384Z"
    },
    "papermill": {
     "duration": 0.575495,
     "end_time": "2022-09-14T15:23:50.111895",
     "exception": false,
     "start_time": "2022-09-14T15:23:49.536400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All column names have been striped, lowered case, replaced space with underscore if any\n",
      "Dropped duplicated instances if any\n",
      "Categorical instances have been striped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45391/3125563000.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>s/g</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بکواس مت</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>0.207547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           tweets  label  s/g  \\\n",
       "0                                                        بکواس مت      1    1   \n",
       "1                   تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو      1    0   \n",
       "2  آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے      1    1   \n",
       "\n",
       "   word_count  char_count  word_density  \n",
       "0           2           7      0.250000  \n",
       "1           9          37      0.236842  \n",
       "2          11          52      0.207547  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trim(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df = df.drop_duplicates()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace(' ','_')\n",
    "    df_obj = df.select_dtypes(['object'])\n",
    "    df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "    print(\"All column names have been striped, lowered case, replaced space with underscore if any\")\n",
    "    print(\"Dropped duplicated instances if any\")\n",
    "    print(\"Categorical instances have been striped\")\n",
    "    return df\n",
    "\n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "df =pd.read_csv('Urdu-threat-detection-FIRE2022/Train_Clean.csv')\n",
    "# df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df = trim(df)\n",
    "\n",
    "def vc(df, column, r=False):\n",
    "    vc_df = df.reset_index().groupby([column]).size().to_frame('count')\n",
    "    vc_df['percentage (%)'] = vc_df['count'].div(sum(vc_df['count'])).mul(100)\n",
    "    vc_df = vc_df.sort_values(by=['percentage (%)'], ascending=False)\n",
    "    if r:\n",
    "        return vc_df\n",
    "    else:\n",
    "        print(f'STATUS: Value counts of \"{column}\"...')\n",
    "        display(vc_df)\n",
    "        \n",
    "def shape(df,df_name):\n",
    "    print(f'STATUS: Dimension of \"{df_name}\" = {df.shape}')\n",
    "        \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8526b3a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:50.337836Z",
     "iopub.status.busy": "2022-09-14T15:23:50.336912Z",
     "iopub.status.idle": "2022-09-14T15:23:50.358282Z",
     "shell.execute_reply": "2022-09-14T15:23:50.357036Z"
    },
    "papermill": {
     "duration": 0.04167,
     "end_time": "2022-09-14T15:23:50.361128",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.319458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word count of all tweets is: 47869\n"
     ]
    }
   ],
   "source": [
    "total_word_count = df['tweets'].str.split().str.len().sum()\n",
    "print(f'The word count of all tweets is: {int(total_word_count)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c132d",
   "metadata": {
    "papermill": {
     "duration": 0.016344,
     "end_time": "2022-09-14T15:23:50.482133",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.465789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>1.4 <span style='color:red'>|</span> General Cleaning </b> <a class=\"anchor\" id=\"1.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff55e92b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:50.517809Z",
     "iopub.status.busy": "2022-09-14T15:23:50.516978Z",
     "iopub.status.idle": "2022-09-14T15:23:50.528847Z",
     "shell.execute_reply": "2022-09-14T15:23:50.527111Z"
    },
    "papermill": {
     "duration": 0.032854,
     "end_time": "2022-09-14T15:23:50.531444",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.498590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape is: (3560, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweets          0\n",
       "label           0\n",
       "s/g             0\n",
       "word_count      0\n",
       "char_count      0\n",
       "word_density    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to print data shape\n",
    "print(f'data shape is: {df.shape}')\n",
    "\n",
    "# to identify the null values by descending order\n",
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5853c0b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:50.601747Z",
     "iopub.status.busy": "2022-09-14T15:23:50.601308Z",
     "iopub.status.idle": "2022-09-14T15:23:50.626532Z",
     "shell.execute_reply": "2022-09-14T15:23:50.624863Z"
    },
    "papermill": {
     "duration": 0.046357,
     "end_time": "2022-09-14T15:23:50.629204",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.582847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3560 entries, 0 to 3563\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   tweets        3560 non-null   object \n",
      " 1   label         3560 non-null   int64  \n",
      " 2   s/g           3560 non-null   int64  \n",
      " 3   word_count    3560 non-null   int64  \n",
      " 4   char_count    3560 non-null   int64  \n",
      " 5   word_density  3560 non-null   float64\n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 194.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# to remove transcription rows that is empty\n",
    "df = df[df['tweets'].notna()]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eca585",
   "metadata": {
    "papermill": {
     "duration": 0.017365,
     "end_time": "2022-09-14T15:23:50.663931",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.646566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After dropping the null values, there are no null values for the tweets attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2644b3c",
   "metadata": {
    "papermill": {
     "duration": 0.01757,
     "end_time": "2022-09-14T15:23:50.820304",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.802734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>2.0 <span style='color:red'>|</span> Text Normalisation </b> <a class=\"anchor\" id=\"2.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb66b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T13:39:00.032844Z",
     "iopub.status.busy": "2022-09-14T13:39:00.032363Z",
     "iopub.status.idle": "2022-09-14T13:39:00.043819Z",
     "shell.execute_reply": "2022-09-14T13:39:00.041823Z",
     "shell.execute_reply.started": "2022-09-14T13:39:00.032806Z"
    },
    "papermill": {
     "duration": 0.01752,
     "end_time": "2022-09-14T15:23:50.855738",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.838218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Data normalisation will be conducted for the trascription. One of the reasons is to convert the transcript into standard format, which important for data extraction later. In this data normalisation task, following task will be executed, which are:\n",
    "1. Lowe Case\n",
    "2. Removing punctuation and numbers\n",
    "3. Tokenisation of the transcription\n",
    "4. Lemmatisation\n",
    "5. Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519056a0",
   "metadata": {
    "papermill": {
     "duration": 0.018214,
     "end_time": "2022-09-14T15:23:50.891691",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.873477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>2.1 <span style='color:red'>|</span> Lower Case </b> <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb823ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:50.929371Z",
     "iopub.status.busy": "2022-09-14T15:23:50.928969Z",
     "iopub.status.idle": "2022-09-14T15:23:50.942426Z",
     "shell.execute_reply": "2022-09-14T15:23:50.941508Z"
    },
    "papermill": {
     "duration": 0.035314,
     "end_time": "2022-09-14T15:23:50.944848",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.909534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>s/g</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بکواس مت</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>0.207547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           tweets  label  s/g  \\\n",
       "0                                                        بکواس مت      1    1   \n",
       "1                   تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو      1    0   \n",
       "2  آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے      1    1   \n",
       "\n",
       "   word_count  char_count  word_density  \n",
       "0           2           7      0.250000  \n",
       "1           9          37      0.236842  \n",
       "2          11          52      0.207547  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To convert transcription into lowercase\n",
    "def lower(df, attribute):\n",
    "    df.loc[:,attribute] = df[attribute].apply(lambda x : str.lower(x))\n",
    "    return df\n",
    "df = lower(df,'tweets')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7638218",
   "metadata": {
    "papermill": {
     "duration": 0.01909,
     "end_time": "2022-09-14T15:23:50.982570",
     "exception": false,
     "start_time": "2022-09-14T15:23:50.963480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>2.2 <span style='color:red'>|</span> Remove Punctuation and Numbers </b> <a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b22128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:51.022012Z",
     "iopub.status.busy": "2022-09-14T15:23:51.020836Z",
     "iopub.status.idle": "2022-09-14T15:23:51.102125Z",
     "shell.execute_reply": "2022-09-14T15:23:51.100857Z"
    },
    "papermill": {
     "duration": 0.104002,
     "end_time": "2022-09-14T15:23:51.104854",
     "exception": false,
     "start_time": "2022-09-14T15:23:51.000852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>s/g</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بکواس مت</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>0.207547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           tweets  label  s/g  \\\n",
       "0                                                        بکواس مت      1    1   \n",
       "1                   تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو      1    0   \n",
       "2  آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے      1    1   \n",
       "\n",
       "   word_count  char_count  word_density  \n",
       "0           2           7      0.250000  \n",
       "1           9          37      0.236842  \n",
       "2          11          52      0.207547  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To remove transcription punctuation and numbers\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "def remove_punc_num(df, attribute):\n",
    "    df.loc[:,attribute] = df[attribute].apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "    df[attribute] = df[attribute].str.replace('\\d+', '')\n",
    "    return df\n",
    "df =remove_punc_num(df, 'tweets')\n",
    "df_no_punc =df.copy()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45781c44",
   "metadata": {
    "papermill": {
     "duration": 0.019048,
     "end_time": "2022-09-14T15:23:51.142314",
     "exception": false,
     "start_time": "2022-09-14T15:23:51.123266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>2.3 <span style='color:red'>|</span> Tokenisation </b> <a class=\"anchor\" id=\"2.3\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97ff0916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:51.180949Z",
     "iopub.status.busy": "2022-09-14T15:23:51.180504Z",
     "iopub.status.idle": "2022-09-14T15:23:51.261621Z",
     "shell.execute_reply": "2022-09-14T15:23:51.260193Z"
    },
    "papermill": {
     "duration": 0.103198,
     "end_time": "2022-09-14T15:23:51.264114",
     "exception": false,
     "start_time": "2022-09-14T15:23:51.160916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>s/g</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>tokenised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بکواس مت</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[بکواس, مت]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>[تمہاری, ہیجڑا, فورس, نہتے, سامنے, بکری, بنی, ڈوب, مرو]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>[آفیسر, سمیتبھارتی, فوجی, جہنم, واصلبنکرز, تباہ, بھارت, کو, پتہ, لگ, جائے]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           tweets  label  s/g  \\\n",
       "0                                                        بکواس مت      1    1   \n",
       "1                   تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو      1    0   \n",
       "2  آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے      1    1   \n",
       "\n",
       "   word_count  char_count  word_density  \\\n",
       "0           2           7      0.250000   \n",
       "1           9          37      0.236842   \n",
       "2          11          52      0.207547   \n",
       "\n",
       "                                                                    tokenised  \n",
       "0                                                                 [بکواس, مت]  \n",
       "1                     [تمہاری, ہیجڑا, فورس, نہتے, سامنے, بکری, بنی, ڈوب, مرو]  \n",
       "2  [آفیسر, سمیتبھارتی, فوجی, جہنم, واصلبنکرز, تباہ, بھارت, کو, پتہ, لگ, جائے]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to tokenise transcription\n",
    "\n",
    "# import nltk\n",
    "tk =WhitespaceTokenizer()\n",
    "def tokenise(df, attribute):\n",
    "    df['tokenised'] = df.apply(lambda row: tk.tokenize(str(row[attribute])), axis=1)\n",
    "    return df\n",
    "df =tokenise(df, 'tweets')\n",
    "df_experiment =df.copy()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3b998",
   "metadata": {
    "papermill": {
     "duration": 0.017762,
     "end_time": "2022-09-14T15:23:53.340178",
     "exception": false,
     "start_time": "2022-09-14T15:23:53.322416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>2.5 <span style='color:red'>|</span> Stop Words Removal </b> <a class=\"anchor\" id=\"2.5\"></a>\n",
    "\n",
    "\n",
    "Removing stop words from the feature space, otherwise it will affect the classifier performance as the collection frequency is often high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183d17d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:53.378785Z",
     "iopub.status.busy": "2022-09-14T15:23:53.378293Z",
     "iopub.status.idle": "2022-09-14T15:23:53.393454Z",
     "shell.execute_reply": "2022-09-14T15:23:53.391688Z"
    },
    "papermill": {
     "duration": 0.037582,
     "end_time": "2022-09-14T15:23:53.396083",
     "exception": false,
     "start_time": "2022-09-14T15:23:53.358501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 548 stop words \n",
      "\n",
      "['ثھی', 'خو', 'گی', 'اپٌے', 'گئے', 'ثہت', 'طرف', 'ہوبری', 'پبئے', 'اپٌب', 'دوضری', 'گیب', 'کت', 'گب', 'ثھی', 'ضے', 'ہر', 'پر', 'اش', 'دی', 'گے', 'لگیں', 'ہے', 'ثعذ', 'ضکتے', 'وٍ', 'تھی', 'اى', 'دیب', 'لئے', 'والے', 'یہ', 'ثدبئے', 'ضکتی', 'ًے', 'تھب', 'اًذر', 'رریعے', 'لگی', 'ہوبرا', 'ہوًے', 'ثبہر', 'ضکتب', 'ًہیں', 'تو', 'اور', 'رہب', 'لگے', 'ہوضکتب', 'ہوں', 'کب', 'ہوبرے', 'توبم', 'کیب', 'ایطے', 'رہی', 'هگر', 'ہوضکتی', 'ہیں', 'کریں', 'ہو', 'تک', 'کی', 'ایک', 'رہے', 'هیں', 'ہوضکتے', 'کیطے', 'ہوًب', 'تت', 'کہ', 'ہوا', 'آئے', 'ضبتھ', 'ًب', 'تھے', 'کیوں', 'ہوتب', 'ًہ', 'خت', 'کے', 'پھر', 'ثغیر', 'خبرہے', 'رکھ', 'کیطب', 'کوئی', 'رریعے', 'ثبرے', 'خب', 'اضطرذ', 'ثلکہ', 'خجکہ', 'رکھتب', 'کیطرف', 'ثراں', 'خبرہب', 'رریعہ', 'کطی', 'اضکب', 'ثٌذ', 'خص', 'رکھتبہوں', 'کیلئے', 'ثبئیں', 'توہیں', 'دوضرے', 'کررہی', 'اضکی', 'ثیچ', 'خوکہ', 'رکھتی', 'کیوًکہ', 'دوًوں', 'کررہے', 'خبرہی', 'ثرآں', 'اضکے', 'پچھلا', 'خیطب', 'رکھتے', 'کےثعذ', 'توہی', 'دوراى', 'کررہب', 'یہبں', 'ٓ آش', 'اًہیں', 'ثي', 'پطٌذ', 'تھوڑا', 'چکے', 'زکویہ', 'دوضروں', 'ضکب', 'اة', 'اوًچب', 'ثٌب', 'پل', 'تھوڑی', 'چلا', 'خبهوظ', 'دیتب', 'ضکٌب', 'اخبزت', 'اوًچبئی', 'ثٌبرہب', 'پوچھب', 'تھوڑے', 'چلو', 'ختن', 'دیتی', 'ضکی', 'اچھب', 'اوًچی', 'ثٌبرہی', 'پوچھتب', 'تیي', 'چلیں', 'در', 'دیتے', 'ضکے', 'اچھی', 'اوًچے', 'ثٌبرہے', 'پوچھتی', 'خبًب', 'چلے', 'درخبت', 'دیر', 'ضلطلہ', 'اچھے', 'اٹھبًب', 'ثٌبًب', 'پوچھتے', 'خبًتب', 'چھوٹب', 'درخہ', 'دیکھٌب', 'ضوچ', 'اختتبم', 'اہن', 'ثٌذ', 'پوچھٌب', 'خبًتی', 'چھوٹوں', 'درخے', 'دیکھو', 'ضوچب', 'ادھر', 'آئی', 'ثٌذکرًب', 'پوچھو', 'خبًتے', 'چھوٹی', 'درزقیقت', 'دیکھی', 'ضوچتب', 'ارد', 'آئے \\tثٌذکرو', 'پوچھوں', 'خبًٌب', 'چھوٹے', 'درضت', 'دیکھیں', 'ضوچتی', 'اردگرد', 'آج', 'ثٌذی', 'پوچھیں', 'خططرذ', 'چھہ', 'دش', 'دیٌب', 'ضوچتے', 'ارکبى', 'آخر', 'ثڑا', 'پورا', 'خگہ', 'چیسیں', 'دفعہ', 'دے', 'ضوچٌب', 'اضتعوبل', 'آخرکبر', 'ثڑوں', 'پہلا', 'خگہوں', 'زبصل', 'دکھبئیں', 'راضتوں', 'ضوچو', 'اضتعوبلات', 'آدهی', 'ثڑی', 'پہلی', 'خگہیں', 'زبضر', 'دکھبتب', 'راضتہ', 'ضوچی', 'اغیب', 'آًب', 'ثڑے', 'پہلےضی', 'خلذی', 'زبل', 'دکھبتی', 'راضتے', 'ضوچیں', 'اطراف', 'آٹھ', 'ثھر', 'پہلےضے', 'خٌبة', 'زبل', 'دکھبتے', 'رکي', 'ضیذھب', 'افراد', 'آیب', 'ثھرا', 'پہلےضےہی', 'خواى', 'زبلات', 'دکھبًب', 'رکھب', 'ضیذھی', 'اکثر', 'ثب', 'ثھراہوا', 'پیع', 'خوًہی', 'زبلیہ', 'دکھبو', 'رکھی', 'ضیذھے', 'اکٹھب', 'ثبترتیت', 'ثھرپور', 'تبزٍ', 'خیطبکہ', 'زصوں', 'دکھبیب', 'رکھے', 'ضیکٌڈ', 'اکٹھی', 'ثبری', 'ثہتر', 'تر', 'چبر', 'زصہ', 'دلچطپ', 'زیبدٍ', 'غبیذ', 'اکٹھے', 'ثبلا', 'ثہتری', 'ترتیت', 'چبہب', 'زصے', 'دلچطپی', 'ضبت', 'غخص', 'اکیلا', 'ثبلترتیت', 'ثہتریي', 'تریي', 'چبہٌب', 'زقبئق', 'دلچطپیبں', 'ضبدٍ', 'غذ', 'اکیلی', 'ثرش', 'پبش', 'تعذاد', 'چبہے', 'زقیتیں', 'هٌبضت', 'ضبرا', 'غروع', 'اکیلے', 'ثغیر', 'پبًب', 'اً', 'ب', 'ج', 'ی', 'ر', 'ق', 'ت', 'چکب', 'زقیقت', 'دو', 'ضبرے', 'غروعبت', 'اگرچہ', 'ثلٌذ', 'پبًچ', 'تن', 'چکی', 'زکن', 'دور', 'ضبل', 'غے', 'الگ', 'ثلٌذوثبلا', 'پراًب', 'تٌہب', 'چکیں', 'اً', 'ب', 'و', 'ک', 'ز', 'دوضرا', 'ضبلوں', 'صبف', 'صسیر', 'قجیلہ', 'کوًطے', 'لازهی', 'هطئلے', 'ًیب', 'طریق', 'کرتی', 'کہتے', 'صفر', 'قطن', 'کھولا', 'لگتب', 'هطبئل', 'وار', 'طریقوں', 'کرتے', 'کہٌب', 'صورت', 'کئی', 'کھولٌب', 'لگتی', 'هطتعول', 'وار', 'طریقہ', 'کرتےہو', 'کہٌب', 'صورتسبل', 'کئے', 'کھولو', 'لگتے', 'هػتول', 'ٹھیک', 'طریقے', 'کرًب', 'کہو', 'صورتوں', 'کبفی', 'کھولی \\tلگٌب', 'هطلق', 'ڈھوًڈا', 'طور', 'کرو', 'کہوں', 'صورتیں', 'کبم', 'کھولیں', 'لگی', 'هعلوم', 'ڈھوًڈلیب', 'طورپر', 'کریں', 'کہی', 'ضرور', 'کجھی', 'کھولے', 'لگے', 'هکول', 'ڈھوًڈًب', 'ظبہر', 'کرے', 'کہیں', 'ضرورت', 'کرا', 'کہب', 'لوجب', 'هلا', 'ڈھوًڈو', 'عذد', 'کل', 'کہیں', 'اً', 'ب', 'ت', 'ر', 'و', 'ر', 'ض', 'کرتب', 'کہتب', 'لوجی', 'هوکي', 'ڈھوًڈی', 'عظین', 'کن', 'کہے', 'ضروری', 'کرتبہوں', 'کہتی', 'لوجے', 'هوکٌبت', 'ڈھوًڈیں', 'علاقوں', 'کوتر', 'کیے', 'لوسبت', 'هوکٌہ', 'ہن', 'لے', 'ًبپطٌذ', 'ہورہے', 'علاقہ', 'کورا', 'کےرریعے', 'لوسہ', 'هڑا', 'ہوئی', 'هتعلق', 'ًبگسیر', 'ہوگئی', 'علاقے', 'کوروں', 'گئی', 'لو', 'هڑًب', 'ہوئے', 'هسترم', 'ًطجت', 'ہوگئے', 'علاوٍ', 'کورٍ', 'گرد', 'لوگ', 'هڑے', 'ہوتی', 'هسترهہ', 'ًقطہ', 'ہوگیب', 'اً', 'ب', 'ه', 'و', 'و', 'ع', 'کورے', 'گروپ', 'لوگوں', 'هہرثبى', 'ہوتے', 'هسطوش', 'ًکبلٌب', 'ہوًی', 'عووهی', 'کوطي', 'گروٍ', 'لڑکپي', 'هیرا', 'ہوچکب', 'هختلف', 'ًکتہ', 'ہی', 'فرد', 'کوى', 'گروہوں', 'لی', 'هیری', 'ہوچکی', 'هسیذ', 'ًو', 'اً', 'ب', 'ی', 'ق', 'ی', 'فی', 'کوًطب', 'گٌتی', 'لیب', 'هیرے', 'ہوچکے', 'هطئلہ', 'ًوخواى', 'یقیٌی', 'قجل', 'کوًطی', 'اً', 'ب', 'ه', 'ز', 'لا', 'لیٌب', 'ًئی', 'ہورہب', 'لیں', 'ًئے', 'ہورہی', 'ثبعث', 'ضت']\n"
     ]
    }
   ],
   "source": [
    "# Showing the list of the English stop words, it has a number of 179 stop words in this list\n",
    "\n",
    "stop = stopwords.words('urdu')\n",
    "print(f\"There are {len(stop)} stop words \\n\")\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fba95fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:53.434843Z",
     "iopub.status.busy": "2022-09-14T15:23:53.434026Z",
     "iopub.status.idle": "2022-09-14T15:23:53.785031Z",
     "shell.execute_reply": "2022-09-14T15:23:53.783838Z"
    },
    "papermill": {
     "duration": 0.373189,
     "end_time": "2022-09-14T15:23:53.787877",
     "exception": false,
     "start_time": "2022-09-14T15:23:53.414688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>s/g</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>tokenised</th>\n",
       "      <th>stemmed_without_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بکواس مت</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[بکواس, مت]</td>\n",
       "      <td>بکواس مت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>[تمہاری, ہیجڑا, فورس, نہتے, سامنے, بکری, بنی, ڈوب, مرو]</td>\n",
       "      <td>تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweets  label  s/g  word_count  \\\n",
       "0                                       بکواس مت      1    1           2   \n",
       "1  تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو      1    0           9   \n",
       "\n",
       "   char_count  word_density  \\\n",
       "0           7      0.250000   \n",
       "1          37      0.236842   \n",
       "\n",
       "                                                 tokenised  \\\n",
       "0                                              [بکواس, مت]   \n",
       "1  [تمہاری, ہیجڑا, فورس, نہتے, سامنے, بکری, بنی, ڈوب, مرو]   \n",
       "\n",
       "                            stemmed_without_stop  \n",
       "0                                       بکواس مت  \n",
       "1  تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stop words\n",
    "def remove_stop_words(df, attribute):\n",
    "    stop = stopwords.words('english')\n",
    "    df['stemmed_without_stop'] = df[attribute].apply(lambda x: ' '.join([word for word in x if word not in (stop)]))\n",
    "    return df\n",
    "df = remove_stop_words(df, 'tokenised')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c62e1",
   "metadata": {
    "papermill": {
     "duration": 0.018558,
     "end_time": "2022-09-14T15:23:53.825496",
     "exception": false,
     "start_time": "2022-09-14T15:23:53.806938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After the 5 data normalisation steps, each transcription record is now in a standard format, which is ready for the n-gram features extraction later. Hence, we should use the attribute 'stemmed_withou_stop' as the predictor attribute and drop other redundant attributes, namely 'transcription', 'tokenized_transcription' and 'stemmed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba43795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:53.865449Z",
     "iopub.status.busy": "2022-09-14T15:23:53.864912Z",
     "iopub.status.idle": "2022-09-14T15:23:53.879382Z",
     "shell.execute_reply": "2022-09-14T15:23:53.878139Z"
    },
    "papermill": {
     "duration": 0.037635,
     "end_time": "2022-09-14T15:23:53.881995",
     "exception": false,
     "start_time": "2022-09-14T15:23:53.844360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>s/g</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>stemmed_without_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>بکواس مت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>غدار منافق میر اللہ تمہیں زلیل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>83</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>اگست آپ بھارت کو کیا پیغام دینا چاہیں میرا پیغام ھے کشمیر سے نکل جاو ورنہ کتے موت مارے جاو کشمیر ہمارا ھے</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  s/g  word_count  char_count  word_density  \\\n",
       "0      1    1           2           7      0.250000   \n",
       "1      1    0           9          37      0.236842   \n",
       "2      1    1          11          52      0.207547   \n",
       "3      1    1           6          25      0.230769   \n",
       "4      1    0          23          83      0.273810   \n",
       "\n",
       "                                                                                        stemmed_without_stop  \n",
       "0                                                                                                   بکواس مت  \n",
       "1                                                              تمہاری ہیجڑا فورس نہتے سامنے بکری بنی ڈوب مرو  \n",
       "2                                             آفیسر سمیتبھارتی فوجی جہنم واصلبنکرز تباہ بھارت کو پتہ لگ جائے  \n",
       "3                                                                             غدار منافق میر اللہ تمہیں زلیل  \n",
       "4  اگست آپ بھارت کو کیا پیغام دینا چاہیں میرا پیغام ھے کشمیر سے نکل جاو ورنہ کتے موت مارے جاو کشمیر ہمارا ھے  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =df.drop(['tweets', 'tokenised'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35f893c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"s/g\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe965aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:53.923090Z",
     "iopub.status.busy": "2022-09-14T15:23:53.922163Z",
     "iopub.status.idle": "2022-09-14T15:23:53.944737Z",
     "shell.execute_reply": "2022-09-14T15:23:53.943390Z"
    },
    "papermill": {
     "duration": 0.046229,
     "end_time": "2022-09-14T15:23:53.947871",
     "exception": false,
     "start_time": "2022-09-14T15:23:53.901642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word count of tweets after normalised is: 47869\n",
      "0.0% less word\n"
     ]
    }
   ],
   "source": [
    "total_word_count_normalised = df['stemmed_without_stop'].str.split().str.len().sum()\n",
    "print(f'The word count of tweets after normalised is: {int(total_word_count_normalised)}')\n",
    "print(f'{round((total_word_count - total_word_count_normalised)/total_word_count*100, 2)}% less word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7e6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:53.988565Z",
     "iopub.status.busy": "2022-09-14T15:23:53.987677Z",
     "iopub.status.idle": "2022-09-14T15:23:54.001396Z",
     "shell.execute_reply": "2022-09-14T15:23:54.000336Z"
    },
    "papermill": {
     "duration": 0.036127,
     "end_time": "2022-09-14T15:23:54.003906",
     "exception": false,
     "start_time": "2022-09-14T15:23:53.967779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['medical_specialty'])\n",
    "df['encoded_target'] = le.transform(df['medical_specialty'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02fae1a",
   "metadata": {
    "papermill": {
     "duration": 0.018644,
     "end_time": "2022-09-14T15:23:54.042294",
     "exception": false,
     "start_time": "2022-09-14T15:23:54.023650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>3.0 <span style='color:red'>|</span> Text N-Gram Feature Extraction </b> <a class=\"anchor\" id=\"3.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b40dc",
   "metadata": {
    "papermill": {
     "duration": 0.018379,
     "end_time": "2022-09-14T15:23:54.079648",
     "exception": false,
     "start_time": "2022-09-14T15:23:54.061269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use sklearn class 'CountVectoriser' to extract different n-grams features. In order to do so, the transcription should be converted into a list format, rather than a dataframe. For the purpose of converting into a flat list (i.e., there is no inner list), the function of 'flat_list' that defined above is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc2b4b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:54.119861Z",
     "iopub.status.busy": "2022-09-14T15:23:54.118923Z",
     "iopub.status.idle": "2022-09-14T15:23:54.128141Z",
     "shell.execute_reply": "2022-09-14T15:23:54.127147Z"
    },
    "papermill": {
     "duration": 0.032006,
     "end_time": "2022-09-14T15:23:54.130607",
     "exception": false,
     "start_time": "2022-09-14T15:23:54.098601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to flatten one list\n",
    "def flat_list(unflat_list):\n",
    "    flatted = [item for sublist in unflat_list for item in sublist]\n",
    "    return flatted\n",
    "\n",
    "def to_list(df, attribute):\n",
    "    # Select the normalised transcript column \n",
    "    df_transcription = df[[attribute]]\n",
    "    # To convert the attribute into list format, but it has inner list. So it cannot put into the CountVectoriser\n",
    "    unflat_list_transcription = df_transcription.values.tolist()\n",
    "    # Let's use back the function defined above, \"flat_list\", to flatten the list\n",
    "    flat_list_transcription = flat_list(unflat_list_transcription)\n",
    "    return flat_list_transcription\n",
    "flat_list_transcription = to_list(df, 'stemmed_without_stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf7e79",
   "metadata": {
    "papermill": {
     "duration": 0.018217,
     "end_time": "2022-09-14T15:23:54.168409",
     "exception": false,
     "start_time": "2022-09-14T15:23:54.150192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>3.1 <span style='color:red'>|</span> Extract 5 Types of N-Gram </b> <a class=\"anchor\" id=\"3.1\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ef2a5",
   "metadata": {
    "papermill": {
     "duration": 0.018249,
     "end_time": "2022-09-14T15:23:54.204990",
     "exception": false,
     "start_time": "2022-09-14T15:23:54.186741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "CountVectorizer is used to convert a collection of transcript documents to a matrix of n-gram features. To explain the ngram_range, all values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3a9f80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:54.244454Z",
     "iopub.status.busy": "2022-09-14T15:23:54.243376Z",
     "iopub.status.idle": "2022-09-14T15:23:54.252782Z",
     "shell.execute_reply": "2022-09-14T15:23:54.251625Z"
    },
    "papermill": {
     "duration": 0.031666,
     "end_time": "2022-09-14T15:23:54.255094",
     "exception": false,
     "start_time": "2022-09-14T15:23:54.223428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unigram', 'unigram_bigram', 'bigram', 'bigram_trigram', 'trigram']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_features ={'unigram':(1,1),'unigram_bigram':(1,2),'bigram':(2,2),\\\n",
    "       'bigram_trigram':(2,3),'trigram':(3,3)}\n",
    "feature_name=[]\n",
    "temp=[]\n",
    "for key, values in n_gram_features.items():\n",
    "    temp.append(key)\n",
    "    feature_name.append(key)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d8b8aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:54.295087Z",
     "iopub.status.busy": "2022-09-14T15:23:54.294137Z",
     "iopub.status.idle": "2022-09-14T15:23:56.330020Z",
     "shell.execute_reply": "2022-09-14T15:23:56.329111Z"
    },
    "papermill": {
     "duration": 2.05846,
     "end_time": "2022-09-14T15:23:56.332544",
     "exception": false,
     "start_time": "2022-09-14T15:23:54.274084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_n_gram_features(flat_list_transcription):\n",
    "    temp=[]\n",
    "    for key, values in n_gram_features.items(): \n",
    "        vectorizer = CountVectorizer(ngram_range=values)\n",
    "        vectorizer.fit(flat_list_transcription)\n",
    "        temp.append(vectorizer.transform(flat_list_transcription))\n",
    "    return temp\n",
    "temp = generate_n_gram_features(flat_list_transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca585e",
   "metadata": {
    "papermill": {
     "duration": 0.018379,
     "end_time": "2022-09-14T15:23:56.369384",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.351005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>3.2 <span style='color:red'>|</span> Dimension of Each Feature Vector </b> <a class=\"anchor\" id=\"3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85fcc606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:56.408369Z",
     "iopub.status.busy": "2022-09-14T15:23:56.407710Z",
     "iopub.status.idle": "2022-09-14T15:23:56.422457Z",
     "shell.execute_reply": "2022-09-14T15:23:56.421289Z"
    },
    "papermill": {
     "duration": 0.037391,
     "end_time": "2022-09-14T15:23:56.425128",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.387737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N-Gram Feature Vector</th>\n",
       "      <th>Data Dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unigram</td>\n",
       "      <td>(3560, 7642)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unigram_bigram</td>\n",
       "      <td>(3560, 39164)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bigram</td>\n",
       "      <td>(3560, 31522)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bigram_trigram</td>\n",
       "      <td>(3560, 69252)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trigram</td>\n",
       "      <td>(3560, 37730)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  N-Gram Feature Vector Data Dimension\n",
       "0               unigram   (3560, 7642)\n",
       "1        unigram_bigram  (3560, 39164)\n",
       "2                bigram  (3560, 31522)\n",
       "3        bigram_trigram  (3560, 69252)\n",
       "4               trigram  (3560, 37730)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = {'unigram':temp[0], \n",
    "              'unigram_bigram':temp[1], \n",
    "              'bigram':temp[2], \n",
    "              'bigram_trigram':temp[3], \n",
    "              'trigram':temp[4]}\n",
    "feature_vector = [] ; feature_vector_shape = []\n",
    "for key in dataframes:\n",
    "    feature_vector.append(key)\n",
    "    feature_vector_shape.append(dataframes[key].shape)\n",
    "\n",
    "n_gram_df = pd.DataFrame({'N-Gram Feature Vector':feature_vector, 'Data Dimension':feature_vector_shape})\n",
    "n_gram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbb57e",
   "metadata": {
    "papermill": {
     "duration": 0.018363,
     "end_time": "2022-09-14T15:23:56.462654",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.444291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After the feature extraction process, 5 kinds of n-gram features are extracted. It is interesting to notice that when the number of 'n' getting higher (i.e, n=1:unigram, n=2:bigram, n=3:trigram), there is a higer number of columns. This is due to it is getting harder to find similar features that can be stored in similar column when it has a longer connected words as one featuer. If the feature is unique, it will automatically append additional column to store the feaure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94677a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:56.502022Z",
     "iopub.status.busy": "2022-09-14T15:23:56.501609Z",
     "iopub.status.idle": "2022-09-14T15:23:56.507889Z",
     "shell.execute_reply": "2022-09-14T15:23:56.507017Z"
    },
    "papermill": {
     "duration": 0.028787,
     "end_time": "2022-09-14T15:23:56.510188",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.481401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3560x7642 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 44970 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to retrieve a unigram feature vector\n",
    "dataframes['unigram']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709d830",
   "metadata": {
    "papermill": {
     "duration": 0.018953,
     "end_time": "2022-09-14T15:23:56.547984",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.529031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>4.0 <span style='color:red'>|</span> Text Classification Modelling </b> <a class=\"anchor\" id=\"4.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bd1ab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:56.588186Z",
     "iopub.status.busy": "2022-09-14T15:23:56.587179Z",
     "iopub.status.idle": "2022-09-14T15:23:56.775355Z",
     "shell.execute_reply": "2022-09-14T15:23:56.774113Z"
    },
    "papermill": {
     "duration": 0.211539,
     "end_time": "2022-09-14T15:23:56.778466",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.566927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m random_state_number \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8888\u001b[39m\n\u001b[0;32m---> 15\u001b[0m df_target \u001b[38;5;241m=\u001b[39m\u001b[43mdf_train\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random_state_number =8888\n",
    "df_target =df_train[['label']].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4900c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:56.819310Z",
     "iopub.status.busy": "2022-09-14T15:23:56.818856Z",
     "iopub.status.idle": "2022-09-14T15:23:56.830856Z",
     "shell.execute_reply": "2022-09-14T15:23:56.829580Z"
    },
    "papermill": {
     "duration": 0.035786,
     "end_time": "2022-09-14T15:23:56.833296",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.797510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'f1':[f1_score, 'f1'], \n",
    "    'precision': [precision_score, 'precision_macro'], \n",
    "    'recall': [recall_score, 'recall_macro'],\n",
    "    \"roc_auc\":[roc_auc_score,\"roc_auc_macro\"]\n",
    "}\n",
    "\n",
    "# get evaluation result\n",
    "\n",
    "def get_performance(param_grid, base_estimator, dataframes):\n",
    "    df_name_list =[]; best_estimator_list=[]; best_score_list=[]; test_predict_result_list=[];\n",
    "    metric_list = [];\n",
    "    \n",
    "    for df_name, df in dataframes.items():\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(df, df_target, test_size=0.2, random_state=random_state_number)\n",
    "        for _, metric_dict in metrics.items():\n",
    "            sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5, scoring=metric_dict[1],random_state=random_state_number,\n",
    "                                      factor=2).fit(X_train, y_train)\n",
    "\n",
    "            best_estimator = sh.best_estimator_\n",
    "            clf = best_estimator.fit(X_train, y_train)\n",
    "            prediction = clf.predict(X_test)\n",
    "            test_predict_result = metric_dict[0](y_test, prediction, average='macro')\n",
    "\n",
    "            df_name_list.append(df_name) ; best_estimator_list.append(best_estimator) ; \n",
    "            best_score_list.append(sh.best_score_) ; \n",
    "            test_predict_result_list.append(test_predict_result) ;metric_list.append(metric_dict[1])\n",
    "            \n",
    "            \n",
    "    model_result = pd.DataFrame({'Vector':df_name_list,'Metric':metric_list,\n",
    "                               'Calibrated Estimator':best_estimator_list,\n",
    "                               'Best CV Metric Score':best_score_list, 'Test Predict Metric Score': test_predict_result_list})\n",
    "    return model_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2218d",
   "metadata": {
    "papermill": {
     "duration": 0.019203,
     "end_time": "2022-09-14T15:23:56.872396",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.853193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>4.1 <span style='color:red'>|</span> Visualising Classification Prediction </b> <a class=\"anchor\" id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "044ba7f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:56.912117Z",
     "iopub.status.busy": "2022-09-14T15:23:56.911717Z",
     "iopub.status.idle": "2022-09-14T15:23:56.930597Z",
     "shell.execute_reply": "2022-09-14T15:23:56.929307Z"
    },
    "papermill": {
     "duration": 0.042129,
     "end_time": "2022-09-14T15:23:56.933329",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.891200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "font = {'family' : 'Tahoma',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "def vis_classification(vector_type = 'unigram', estimator = KNeighborsClassifier(n_neighbors=9)):\n",
    "    pca = PCA(n_components=2)\n",
    "    df1 = pca.fit_transform(dataframes[vector_type].todense())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df1, df_target, test_size=0.2, random_state=random_state_number)\n",
    "    \n",
    "    # get training set\n",
    "    df2 = pd.DataFrame({'pca1':X_train[:,1], 'pca2': X_train[:,0], 'y':le.inverse_transform(y_train)})\n",
    "    min_1, max_1 = df2['pca1'].min(), df2['pca1'].max()\n",
    "    min_2, max_2 = df2['pca2'].min(), df2['pca2'].max()\n",
    "    \n",
    "    # generate dimension reduced, but extended data\n",
    "    pca1_range = np.linspace(min_1,max_1,30)\n",
    "    pca2_range = np.linspace(min_2,max_2,30)\n",
    "    \n",
    "    # shuffle\n",
    "    np.random.shuffle(pca1_range) ; np.random.shuffle(pca2_range)\n",
    "    \n",
    "    # to dataframe\n",
    "    prediction_test = pd.DataFrame({'pca1':pca1_range, 'pca2':pca2_range})\n",
    "\n",
    "    best_estimator = estimator\n",
    "    \n",
    "    # fit training set and predict extended data\n",
    "    clf = best_estimator.fit(X_train, y_train)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize=(15,6))\n",
    "    cmap = plt.cm.get_cmap('tab10', 4)\n",
    "    fig.suptitle(f\"Visualising {type(estimator).__name__} on {vector_type.capitalize()} Vector\", fontsize=14,fontweight='bold')\n",
    "\n",
    "\n",
    "    def plot_scatter(ax, predictor_set, target, title):\n",
    "        \n",
    "        # plot area classifier\n",
    "        clf = best_estimator.fit(X_train, y_train)\n",
    "        axs[0].tricontourf(X_train[:,0], X_train[:,1], clf.predict(X_train), levels=np.arange(-0.5, 4), zorder=10, alpha=0.3, cmap=cmap, edgecolors=\"k\")\n",
    "        \n",
    "        axs[1].tricontourf(X_test[:,0], X_test[:,1], clf.predict(X_test), levels=np.arange(-0.5, 4), zorder=10, alpha=0.3, cmap=cmap, edgecolors=\"k\")\n",
    "        \n",
    "        # plot scatter\n",
    "        df3 = pd.DataFrame({'pca1':predictor_set[:,1], 'pca2': predictor_set[:,0], 'y':le.inverse_transform(target)})\n",
    "        for y_label in df3['y'].unique():\n",
    "            df_filter = df3[df3['y']==y_label]\n",
    "            ax.scatter(df_filter['pca1'], df_filter['pca2'], alpha=1,label=f\"{y_label}\")\n",
    "        ax.legend()\n",
    "        ax.set_title(f'{title} ({predictor_set.shape[0]} Samples)',fontweight='bold')\n",
    "    plot_scatter(axs[0], X_train, y_train, 'Training Set')\n",
    "    plot_scatter(axs[1], X_test, y_test, 'Testing Set')\n",
    "    axs[0].sharey(axs[1])\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37bf7a21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:23:56.974258Z",
     "iopub.status.busy": "2022-09-14T15:23:56.973792Z",
     "iopub.status.idle": "2022-09-14T15:44:05.253977Z",
     "shell.execute_reply": "2022-09-14T15:44:05.252615Z"
    },
    "papermill": {
     "duration": 1208.323923,
     "end_time": "2022-09-14T15:44:05.276854",
     "exception": false,
     "start_time": "2022-09-14T15:23:56.952931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m35\u001b[39m,\u001b[38;5;241m37\u001b[39m,\u001b[38;5;241m38\u001b[39m,\u001b[38;5;241m39\u001b[39m,\u001b[38;5;241m40\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m150\u001b[39m,\u001b[38;5;241m170\u001b[39m,\u001b[38;5;241m180\u001b[39m,\u001b[38;5;241m190\u001b[39m,\u001b[38;5;241m200\u001b[39m]}\n\u001b[1;32m      2\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state_number)\n\u001b[0;32m----> 3\u001b[0m rfc_result \u001b[38;5;241m=\u001b[39m \u001b[43mget_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m rfc_result\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mget_performance\u001b[0;34m(param_grid, base_estimator, dataframes)\u001b[0m\n\u001b[1;32m     16\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(df, df_target, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mrandom_state_number)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, metric_dict \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 18\u001b[0m     sh \u001b[38;5;241m=\u001b[39m \u001b[43mHalvingGridSearchCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     best_estimator \u001b[38;5;241m=\u001b[39m sh\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     22\u001b[0m     clf \u001b[38;5;241m=\u001b[39m best_estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search_successive_halving.py:261\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_parameters(\n\u001b[1;32m    254\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    255\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    256\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples_orig \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_index_]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search_successive_halving.py:366\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    359\u001b[0m     cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checked_cv_orig\n\u001b[1;32m    361\u001b[0m more_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m: [itr] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_resources\u001b[39m\u001b[38;5;124m\"\u001b[39m: [n_resources] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[1;32m    364\u001b[0m }\n\u001b[0;32m--> 366\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmore_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmore_results\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m n_candidates_to_keep \u001b[38;5;241m=\u001b[39m ceil(n_candidates \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor)\n\u001b[1;32m    371\u001b[0m candidate_params \u001b[38;5;241m=\u001b[39m _top_k(results, n_candidates_to_keep, itr)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:465\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[0;32m--> 465\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    477\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    478\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:466\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[1;32m    465\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    477\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    478\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_base.py:164\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\"\"\"Make and configure a copy of the `base_estimator_` attribute.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mWarning: This method should be used to properly instantiate new\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03msub-estimators.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m estimator \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_estimator_)\n\u001b[0;32m--> 164\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# TODO: Remove in v1.2\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# criterion \"mse\" and \"mae\" would cause warnings in every call to\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# DecisionTreeRegressor.fit(..)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, (DecisionTreeRegressor, ExtraTreeRegressor)):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py:239\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Simple optimization to gain speed (inspect is slow)\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 239\u001b[0m valid_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m nested_params \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)  \u001b[38;5;66;03m# grouped by prefix\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py:210\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03mGet parameters for this estimator.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_param_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    211\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py:192\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimators should always \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecify their parameters in the signature\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m follow this convention.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m, init_signature)\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Extract and sort argument names excluding 'self'\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [None,30,32,35,37,38,39,40],'min_samples_split': [2,150,170,180,190,200]}\n",
    "base_estimator = RandomForestClassifier(random_state=random_state_number)\n",
    "rfc_result = get_performance(param_grid, base_estimator, dataframes)\n",
    "rfc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf51244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:44:05.318715Z",
     "iopub.status.busy": "2022-09-14T15:44:05.317884Z",
     "iopub.status.idle": "2022-09-14T15:44:06.874580Z",
     "shell.execute_reply": "2022-09-14T15:44:06.873563Z"
    },
    "papermill": {
     "duration": 1.580348,
     "end_time": "2022-09-14T15:44:06.876807",
     "exception": false,
     "start_time": "2022-09-14T15:44:05.296459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def get_best_vector_clf(knn_result):\n",
    "\n",
    "    temp = knn_result[knn_result['Metric'] =='f1_macro']\n",
    "    temp2 = temp.iloc[temp['Best CV Metric Score'].idxmax()].to_frame().T\n",
    "    best_vector = temp2['Vector'].values[0]\n",
    "    best_clf = temp2['Calibrated Estimator'].values[0]\\\n",
    "    \n",
    "    return best_vector, best_clf\n",
    "\n",
    "best_vector, best_clf =  get_best_vector_clf(rfc_result)\n",
    "vis_classification(vector_type = best_vector, estimator = best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa56d4b",
   "metadata": {
    "papermill": {
     "duration": 0.020479,
     "end_time": "2022-09-14T15:44:06.918733",
     "exception": false,
     "start_time": "2022-09-14T15:44:06.898254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>4.2 <span style='color:red'>|</span>  Dimensionality Reduction </b> <a class=\"anchor\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f56fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:44:06.965102Z",
     "iopub.status.busy": "2022-09-14T15:44:06.964063Z",
     "iopub.status.idle": "2022-09-14T15:44:06.976469Z",
     "shell.execute_reply": "2022-09-14T15:44:06.975237Z"
    },
    "papermill": {
     "duration": 0.038937,
     "end_time": "2022-09-14T15:44:06.979232",
     "exception": false,
     "start_time": "2022-09-14T15:44:06.940295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_temp = rfc_result[rfc_result['Metric'] =='f1_macro']\n",
    "# df_temp['Calibrated Estimator']\n",
    "vector_rfc = df_temp[['Vector','Calibrated Estimator']].set_index('Vector').to_dict()['Calibrated Estimator']\n",
    "vector_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c2e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:44:07.024434Z",
     "iopub.status.busy": "2022-09-14T15:44:07.023327Z",
     "iopub.status.idle": "2022-09-14T15:44:11.612054Z",
     "shell.execute_reply": "2022-09-14T15:44:11.610872Z"
    },
    "papermill": {
     "duration": 4.61389,
     "end_time": "2022-09-14T15:44:11.614726",
     "exception": false,
     "start_time": "2022-09-14T15:44:07.000836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "supported_columns_dict = {}\n",
    "for df_name, df in dataframes.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataframes[df_name], df_target, test_size=0.2, random_state=random_state_number)\n",
    "\n",
    "    selector = SelectFromModel(estimator=vector_rfc[df_name]).fit(X_train, y_train)\n",
    "    \n",
    "    filter_columns = selector.get_support()\n",
    "    dataframes[df_name] = dataframes[df_name][:, filter_columns]\n",
    "    \n",
    "shape_dim = [] ; df_names = []\n",
    "for df_name, df in dataframes.items():\n",
    "    shape_dim.append(df.shape)\n",
    "    df_names.append(df_name)\n",
    "n_gram_df_dim = pd.DataFrame({'N-Gram Feature Vector':df_names, 'Data Dimension':shape_dim}) \n",
    "n_gram_df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b5b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:44:11.659264Z",
     "iopub.status.busy": "2022-09-14T15:44:11.658872Z",
     "iopub.status.idle": "2022-09-14T15:44:12.029694Z",
     "shell.execute_reply": "2022-09-14T15:44:12.028683Z"
    },
    "papermill": {
     "duration": 0.396009,
     "end_time": "2022-09-14T15:44:12.032204",
     "exception": false,
     "start_time": "2022-09-14T15:44:11.636195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = n_gram_df_dim['N-Gram Feature Vector'].values\n",
    "b4 = [shape[1] for shape in n_gram_df['Data Dimension'].values]\n",
    "af = [shape[1] for shape in n_gram_df_dim['Data Dimension'].values]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, b4, width, label='Before Dimensionality Reduction', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, af, width, label='After Dimensionality Reduction', color='lime')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Number Columns')\n",
    "ax.set_title('Before and After Dimensionality Reduction')\n",
    "ax.set_xticks(x, labels)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=30)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7ee5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:44:12.079754Z",
     "iopub.status.busy": "2022-09-14T15:44:12.079348Z",
     "iopub.status.idle": "2022-09-14T15:44:21.873190Z",
     "shell.execute_reply": "2022-09-14T15:44:21.872209Z"
    },
    "papermill": {
     "duration": 9.820951,
     "end_time": "2022-09-14T15:44:21.875538",
     "exception": false,
     "start_time": "2022-09-14T15:44:12.054587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': [5,7,9,11,13,15,17,19,21]}\n",
    "base_estimator = KNeighborsClassifier()\n",
    "knn_result = get_performance(param_grid, base_estimator, dataframes)\n",
    "knn_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f93af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:44:21.923695Z",
     "iopub.status.busy": "2022-09-14T15:44:21.923256Z",
     "iopub.status.idle": "2022-09-14T15:44:22.575369Z",
     "shell.execute_reply": "2022-09-14T15:44:22.574041Z"
    },
    "papermill": {
     "duration": 0.678345,
     "end_time": "2022-09-14T15:44:22.577927",
     "exception": false,
     "start_time": "2022-09-14T15:44:21.899582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "best_vector, best_clf =  get_best_vector_clf(knn_result)\n",
    "vis_classification(vector_type = best_vector, estimator = best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf288e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:44:22.630206Z",
     "iopub.status.busy": "2022-09-14T15:44:22.628938Z",
     "iopub.status.idle": "2022-09-14T15:45:10.742561Z",
     "shell.execute_reply": "2022-09-14T15:45:10.741255Z"
    },
    "papermill": {
     "duration": 48.165654,
     "end_time": "2022-09-14T15:45:10.768399",
     "exception": false,
     "start_time": "2022-09-14T15:44:22.602745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [None,4,6,7,8,30,32,35],'min_samples_split': [2,3,4,5,35,10,16,20]}\n",
    "base_estimator = DecisionTreeClassifier(random_state=random_state_number)\n",
    "dtc_result = get_performance(param_grid, base_estimator, dataframes)\n",
    "dtc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8c259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:10.820440Z",
     "iopub.status.busy": "2022-09-14T15:45:10.819261Z",
     "iopub.status.idle": "2022-09-14T15:45:11.490811Z",
     "shell.execute_reply": "2022-09-14T15:45:11.489549Z"
    },
    "papermill": {
     "duration": 0.700723,
     "end_time": "2022-09-14T15:45:11.493919",
     "exception": false,
     "start_time": "2022-09-14T15:45:10.793196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "best_vector, best_clf =  get_best_vector_clf(dtc_result)\n",
    "vis_classification(vector_type = best_vector, estimator = best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc505440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:11.551180Z",
     "iopub.status.busy": "2022-09-14T15:45:11.550305Z",
     "iopub.status.idle": "2022-09-14T15:45:11.573888Z",
     "shell.execute_reply": "2022-09-14T15:45:11.572772Z"
    },
    "papermill": {
     "duration": 0.054992,
     "end_time": "2022-09-14T15:45:11.576293",
     "exception": false,
     "start_time": "2022-09-14T15:45:11.521301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_result = pd.concat([knn_result, \n",
    "                      dtc_result,\n",
    "                      rfc_result\n",
    "                      ]\n",
    "                     ).reset_index(drop=True)\n",
    "\n",
    "df_result.groupby(['Metric']).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951f2c3",
   "metadata": {
    "papermill": {
     "duration": 0.025007,
     "end_time": "2022-09-14T15:45:11.627823",
     "exception": false,
     "start_time": "2022-09-14T15:45:11.602816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>4.3 <span style='color:red'>|</span>  Obtain Best Classifier and Feature Vector </b> <a class=\"anchor\" id=\"4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d406ac82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:11.684640Z",
     "iopub.status.busy": "2022-09-14T15:45:11.683871Z",
     "iopub.status.idle": "2022-09-14T15:45:11.691073Z",
     "shell.execute_reply": "2022-09-14T15:45:11.690091Z"
    },
    "papermill": {
     "duration": 0.038558,
     "end_time": "2022-09-14T15:45:11.693584",
     "exception": false,
     "start_time": "2022-09-14T15:45:11.655026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_result(df_result, metric_score):\n",
    "    df_result_t = df_result[df_result.Metric== 'precision_macro']\n",
    "    precision_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n",
    "\n",
    "    df_result_t = df_result[df_result.Metric== 'recall_macro']\n",
    "    recall_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n",
    "    \n",
    "    df_result_t = df_result[df_result.Metric== 'f1_macro']\n",
    "    f1_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n",
    "\n",
    "    return pd.concat([precision_macro_df,recall_macro_df,f1_macro_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34ea08bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:11.750435Z",
     "iopub.status.busy": "2022-09-14T15:45:11.749661Z",
     "iopub.status.idle": "2022-09-14T15:45:11.772942Z",
     "shell.execute_reply": "2022-09-14T15:45:11.771734Z"
    },
    "papermill": {
     "duration": 0.054915,
     "end_time": "2022-09-14T15:45:11.775712",
     "exception": false,
     "start_time": "2022-09-14T15:45:11.720797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vector</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Calibrated Estimator</th>\n",
       "      <th>Best CV Metric Score</th>\n",
       "      <th>Test Predict Metric Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>trigram</td>\n",
       "      <td>precision_macro</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.740739</td>\n",
       "      <td>0.673557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>unigram</td>\n",
       "      <td>recall_macro</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=30, max_features='sqrt', min_samples_split=150,\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=30, max_features='sqrt', min_samples_split=150,\\n                       random_state=1...</td>\n",
       "      <td>0.693874</td>\n",
       "      <td>0.703109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>unigram</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=38, max_features='sqrt',\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=38, max_features='sqrt',\\n                       random_state=1459224502), DecisionTreeClassifier(max_depth=3...</td>\n",
       "      <td>0.698943</td>\n",
       "      <td>0.699417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Vector           Metric  \\\n",
       "13  trigram  precision_macro   \n",
       "32  unigram     recall_macro   \n",
       "30  unigram         f1_macro   \n",
       "\n",
       "                                                                                                                                                                                                                                              Calibrated Estimator  \\\n",
       "13                                                                                                                                                                                                                                          KNeighborsClassifier()   \n",
       "32  (DecisionTreeClassifier(max_depth=30, max_features='sqrt', min_samples_split=150,\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=30, max_features='sqrt', min_samples_split=150,\\n                       random_state=1...   \n",
       "30  (DecisionTreeClassifier(max_depth=38, max_features='sqrt',\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=38, max_features='sqrt',\\n                       random_state=1459224502), DecisionTreeClassifier(max_depth=3...   \n",
       "\n",
       "   Best CV Metric Score Test Predict Metric Score  \n",
       "13             0.740739                  0.673557  \n",
       "32             0.693874                  0.703109  \n",
       "30             0.698943                  0.699417  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_cv_result = get_best_result(df_result, 'Best CV Metric Score')\n",
    "display(best_cv_result)\n",
    "temp = best_cv_result[best_cv_result['Metric'] == 'f1_macro']\n",
    "best_clf = temp['Calibrated Estimator'].values[0]\n",
    "best_vector = temp['Vector'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a00eac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:11.830813Z",
     "iopub.status.busy": "2022-09-14T15:45:11.830049Z",
     "iopub.status.idle": "2022-09-14T15:45:12.082581Z",
     "shell.execute_reply": "2022-09-14T15:45:12.081348Z"
    },
    "papermill": {
     "duration": 0.283746,
     "end_time": "2022-09-14T15:45:12.085345",
     "exception": false,
     "start_time": "2022-09-14T15:45:11.801599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vector</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Calibrated Estimator</th>\n",
       "      <th>Best CV Metric Score</th>\n",
       "      <th>Test Predict Metric Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>unigram_bigram</td>\n",
       "      <td>precision_macro</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=37, max_features='sqrt',\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=37, max_features='sqrt',\\n                       random_state=1459224502), DecisionTreeClassifier(max_depth=3...</td>\n",
       "      <td>0.701219</td>\n",
       "      <td>0.702183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>unigram</td>\n",
       "      <td>recall_macro</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=30, max_features='sqrt', min_samples_split=150,\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=30, max_features='sqrt', min_samples_split=150,\\n                       random_state=1...</td>\n",
       "      <td>0.693874</td>\n",
       "      <td>0.703109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>unigram</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=38, max_features='sqrt',\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=38, max_features='sqrt',\\n                       random_state=1459224502), DecisionTreeClassifier(max_depth=3...</td>\n",
       "      <td>0.698943</td>\n",
       "      <td>0.699417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Vector           Metric  \\\n",
       "34  unigram_bigram  precision_macro   \n",
       "32         unigram     recall_macro   \n",
       "30         unigram         f1_macro   \n",
       "\n",
       "                                                                                                                                                                                                                                              Calibrated Estimator  \\\n",
       "34  (DecisionTreeClassifier(max_depth=37, max_features='sqrt',\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=37, max_features='sqrt',\\n                       random_state=1459224502), DecisionTreeClassifier(max_depth=3...   \n",
       "32  (DecisionTreeClassifier(max_depth=30, max_features='sqrt', min_samples_split=150,\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=30, max_features='sqrt', min_samples_split=150,\\n                       random_state=1...   \n",
       "30  (DecisionTreeClassifier(max_depth=38, max_features='sqrt',\\n                       random_state=1985925507), DecisionTreeClassifier(max_depth=38, max_features='sqrt',\\n                       random_state=1459224502), DecisionTreeClassifier(max_depth=3...   \n",
       "\n",
       "   Best CV Metric Score Test Predict Metric Score  \n",
       "34             0.701219                  0.702183  \n",
       "32             0.693874                  0.703109  \n",
       "30             0.698943                  0.699417  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_result(df_result, 'Test Predict Metric Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f98ef7",
   "metadata": {
    "papermill": {
     "duration": 0.026937,
     "end_time": "2022-09-14T15:45:12.138633",
     "exception": false,
     "start_time": "2022-09-14T15:45:12.111696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b>4.4 <span style='color:red'>|</span>  Evaluate on Each Class Labels </b> <a class=\"anchor\" id=\"4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2eafbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:12.195472Z",
     "iopub.status.busy": "2022-09-14T15:45:12.194744Z",
     "iopub.status.idle": "2022-09-14T15:45:12.218246Z",
     "shell.execute_reply": "2022-09-14T15:45:12.216597Z"
    },
    "papermill": {
     "duration": 0.053495,
     "end_time": "2022-09-14T15:45:12.220712",
     "exception": false,
     "start_time": "2022-09-14T15:45:12.167217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "   Threatening       0.66      0.72      0.69       344\n",
      "Non-Threatning       0.71      0.65      0.68       368\n",
      "\n",
      "      accuracy                           0.68       712\n",
      "     macro avg       0.68      0.68      0.68       712\n",
      "  weighted avg       0.68      0.68      0.68       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataframes[best_vector], df_target, test_size=0.2, \\\n",
    "                                                    random_state=random_state_number)\n",
    "clf = best_clf.fit(X_train, y_train)\n",
    "y_test_pred= clf.predict(X_test)\n",
    "target_names = [\"Threatening\",\"Non-Threatning\"]\n",
    "print(classification_report(y_test,y_test_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01878af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:45:12.278809Z",
     "iopub.status.busy": "2022-09-14T15:45:12.278340Z",
     "iopub.status.idle": "2022-09-14T15:45:12.292348Z",
     "shell.execute_reply": "2022-09-14T15:45:12.290946Z"
    },
    "papermill": {
     "duration": 0.04529,
     "end_time": "2022-09-14T15:45:12.294865",
     "exception": false,
     "start_time": "2022-09-14T15:45:12.249575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "sample_predict = pd.DataFrame({'Actual Y Test': le.inverse_transform(y_test),'Best Prediction':le.inverse_transform(y_test_pred)})\n",
    "sample_predict.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1295.42731,
   "end_time": "2022-09-14T15:45:13.308486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-14T15:23:37.881176",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
